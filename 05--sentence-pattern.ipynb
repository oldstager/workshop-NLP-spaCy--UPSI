{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b72dba2-8387-4e03-bd92-c44b6f74ce75",
   "metadata": {},
   "source": [
    "# Sentence Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34cb4f81-e229-43ce-b5af-6dc3a8595049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We You nsubj nominal subject\n",
      "We You PRON pronoun\n",
      "can must aux auxiliary\n",
      "can must AUX auxiliary\n",
      "overtake specify ROOT root\n",
      "overtake specify VERB verb\n",
      "them it dobj direct object\n",
      "them it PRON pronoun\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "doc1 = nlp(u'We can overtake them.')\n",
    "doc2 = nlp(u'You must specify it.')\n",
    "\n",
    "for i in range(len(doc1)-1):\n",
    "    if doc1[i].dep_ == doc2[i].dep_:\n",
    "        print(doc1[i].text, doc2[i].text, doc1[i].dep_, spacy.explain(doc1[i].dep_))\n",
    "    if doc1[i].pos_ == doc2[i].pos_:\n",
    "        print(doc1[i].text, doc2[i].text, doc1[i].pos_, spacy.explain(doc1[i].pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a92e8d9-959b-40da-8c33-659142175186",
   "metadata": {},
   "source": [
    "We may check whether a sentence has a specific pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95aa5796-6951-4795-b3d5-9d42999bfb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found\n"
     ]
    }
   ],
   "source": [
    "def dep_pattern(doc):\n",
    "    for i in range(len(doc)-1):\n",
    "        if doc[i].dep_ == 'nsubj' and doc[i+1].dep_ == 'aux' and  doc[i+2].dep_ == 'ROOT':\n",
    "            for tok in doc[i+2].children:\n",
    "                if tok.dep_ == 'dobj':\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "if dep_pattern(doc1):\n",
    "  print('Found')\n",
    "else:\n",
    "  print('Not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03af753-ef95-4489-b26e-8713d79a42ee",
   "metadata": {},
   "source": [
    "or we may use *Matcher* to check the pattern: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc2ff3ef-29ef-42ab-8d4c-5ca3e7ae2de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span:  We can overtake\n",
      "The positions in the doc are:  0 - 3\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [[{\"DEP\": \"nsubj\"}, {\"DEP\": \"aux\"}, {\"DEP\": \"ROOT\"}]]\n",
    "\n",
    "matcher.add(\"NsubjAuxRoot\", pattern)\n",
    "\n",
    "matches = matcher(doc1)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    span = doc1[start:end]\n",
    "    print(\"Span: \", span.text)\n",
    "    print(\"The positions in the doc are: \", start, \"-\", end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b96f62-af1b-42bd-8dad-7518bfccdb3f",
   "metadata": {},
   "source": [
    "## Generating Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d654581-bbba-4c61-b72b-55b103e93c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can recognize symbols too.\n"
     ]
    }
   ],
   "source": [
    "def dep_pattern(doc):\n",
    "    for i in range(len(doc)-1):\n",
    "        if doc[i].dep_ == 'nsubj' and doc[i+1].dep_ == 'aux' and  doc[i+2].dep_ == 'ROOT':\n",
    "            for tok in doc[i+2].children:\n",
    "                if tok.dep_ == 'dobj':\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "def pos_pattern(doc):\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'nsubj' and token.tag_ != 'PRP':\n",
    "            return False\n",
    "        if token.dep_ == 'aux' and token.tag_ != 'MD':\n",
    "            return False\n",
    "        if token.dep_ == 'ROOT' and token.tag_ != 'VB':\n",
    "            return False\n",
    "        if token.dep_ == 'dobj' and token.tag_ != 'PRP':\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def pron_pattern(doc):\n",
    "    plural = ['we','us','they','them']\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'dobj' and token.tag_ == 'PRP':\n",
    "            if token.text in plural:\n",
    "                return 'plural'\n",
    "            else:\n",
    "                return 'singular'\n",
    "    return 'not found'\n",
    "\n",
    "def find_noun(sents, num):\n",
    "    if num == 'plural':\n",
    "        taglist = ['NNS','NNPS']\n",
    "    if num == 'singular':\n",
    "        taglist = ['NN','NNP']\n",
    "    for sent in reversed(sents):\n",
    "        for token in sent:\n",
    "            if token.tag_ in taglist:\n",
    "                return token.text\n",
    "    return 'Noun not found'\n",
    "\n",
    "def gen_utterance(doc, noun):\n",
    "    sent = ''\n",
    "    for i,token in enumerate(doc):\n",
    "        if token.dep_ == 'dobj' and token.tag_ == 'PRP':\n",
    "            sent = doc[:i].text + ' ' + noun + ' ' + doc[i+1:len(doc)-2].text + 'too.'\n",
    "            return sent\n",
    "    return 'Failed to generate an utterance' \n",
    "\n",
    "doc = nlp(u'The symbols are clearly distinguishable. I can recognize them promptly.')\n",
    "\n",
    "sents = list(doc.sents)\n",
    "\n",
    "response = ''\n",
    "\n",
    "noun = ''\n",
    "\n",
    "for i, sent in enumerate(sents):\n",
    "    if dep_pattern(sent) and pos_pattern(sent):\n",
    "        noun = find_noun(sents[:i], pron_pattern(sent))\n",
    "        if noun != 'Noun not found':\n",
    "            response = gen_utterance(sents[i],noun)\n",
    "            break\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc501c20-049b-4f20-9d4f-2986b92d0fcd",
   "metadata": {},
   "source": [
    "## Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fadd4f6d-0ad2-48a2-8d72-167798ff7734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems the user wants a ticket to Berlin\n"
     ]
    }
   ],
   "source": [
    "def det_destination(doc):\n",
    "    for i, token in enumerate(doc):\n",
    "        if token.ent_type != 0 and token.ent_type_ == 'GPE':\n",
    "            while True:\n",
    "                token = token.head\n",
    "                if token.text == 'to':\n",
    "                    return doc[i].text\n",
    "                if token.head == token:\n",
    "                    return 'Failed to determine'\n",
    "    return 'Failed to determine'\n",
    "\n",
    "doc = nlp(u'I am going to the conference in Berlin.')\n",
    "\n",
    "dest = det_destination(doc)\n",
    "\n",
    "print('It seems the user wants a ticket to ' + dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cd3fd0-83c6-4fba-9119-eeee48207b03",
   "metadata": {},
   "source": [
    "## Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b47bac0d-1e4f-463e-bba7-1d7225ff5cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The product sales hit 18.6 million units sold.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"The product sales hit a new record in the first quarter, with 18.6 million units sold.\")\n",
    "\n",
    "phrase = ''\n",
    "\n",
    "for token in doc:\n",
    "    if token.pos_ == 'NUM':\n",
    "        while True:\n",
    "            phrase = phrase + ' ' + token.text\n",
    "            token = token.head\n",
    "            if token not in list(token.head.lefts):\n",
    "                phrase = phrase + ' ' + token.text\n",
    "                if list(token.rights):\n",
    "                    phrase = phrase + ' ' + doc[token.i+1:].text\n",
    "                break\n",
    "        break\n",
    "\n",
    "while True:\n",
    "    token = doc[token.i].head\n",
    "    if token.pos_ != 'ADP':\n",
    "        phrase = token.text + phrase\n",
    "    if token.dep_ == 'ROOT':\n",
    "        break\n",
    "\n",
    "for tok in token.lefts:\n",
    "    if tok.dep_ == 'nsubj':\n",
    "        phrase = ' '.join([tok.text for tok in tok.lefts]) + ' ' + tok.text + ' '+ phrase\n",
    "        break\n",
    "\n",
    "print(phrase.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9285e1-fd3d-466b-84d2-768f929f3ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
